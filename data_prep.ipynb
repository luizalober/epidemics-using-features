{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNasCKNb/zLf9squQ05sngx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import calendar\n","import time\n","import glob\n","import os\n","\n","#Disable the 'SettingWithCopyWarning' when adding new columns to an existing dataframe\n","pd.options.mode.chained_assignment = None  # default='warn'\n","\n","#Data cleaning\n","from functools import reduce\n","import unicodedata\n","import re\n","\n","#Ignore warnings about the DataFrameGroupBy() method\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","\n","#For the graphics\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import matplotlib.dates as mdates\n","import matplotlib.patches as mpatches\n","\n","#Feature selection\n","from sklearn.inspection import permutation_importance\n","\n","#Modeling\n","from sklearn.model_selection import cross_validate\n","from sklearn.model_selection import TimeSeriesSplit\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import cross_val_predict\n","from sklearn.model_selection import ParameterGrid   #to implement manual GridSearch\n","\n","#Dynamic time warping: grouping\n","from dtaidistance import dtw\n","from dtaidistance import dtw_ndim"],"metadata":{"id":"37nDIHg_VxzO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Filtering the data by years"],"metadata":{"id":"vV-yi296rQtE"}},{"cell_type":"code","source":["disease = 'dengue'\n","path = 'C:/Users/your_path/' + disease '/'\n","\n","#packs all yearly files in a single data frame.\n","all_files = glob.glob(os.path.join(path, \"*.csv\"))\n","df = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)\n","df.drop(columns='Unnamed: 0', inplace=True)"],"metadata":{"id":"fyiyzbO4RkY2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#save the cities and the amount of years each one contains in its time series.\n","test_years = []\n","test_cities = []\n","removed_cities = 0\n","removed_cities_names = []\n","\n","df_filled = df.copy()\n","df_filled = df_filled.fillna(0)\n","city_counter = 0\n","\n","for city in list(df_filled.cod_city.unique()):\n","  #print('City', city_counter+1, 'of', len(list(df_filled.cod_city.unique())))\n","  swp = df_filled.loc[ (df_filled['cod_city'] == city) ]\n","  years = list( swp.loc[swp['cod_city'] == city].Year.unique() )\n","\n","  test_cities.append(city)\n","  test_years.append(len(years))\n","\n","  city_counter = city_counter+1\n","\n","\n","cities_lenght_data = {'city': test_cities, 'n_years': test_years}\n","cities_lenght = pd.DataFrame(data=cities_lenght_data)\n","cities_lenght.sort_values(by='n_years', ascending=False, inplace=True)\n","\n","\n","df_max_years = pd.DataFrame()\n","cities = cities_lenght.loc[cities_lenght['n_years'] == max(cities_lenght.n_years.unique())]\n","\n","for city in list(cities.city.unique()):\n","  swp = df_filled.loc[df_filled['cod_city'] == city]\n","  if (len(swp) !=0):\n","    df_max_years = pd.concat([df_max_years, swp])"],"metadata":{"id":"kWpDKj3y3Njm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Adding lags"],"metadata":{"id":"D0pgYEPw4Dua"}},{"cell_type":"code","source":["y_disease = df_max_years['cases']\n","\n","df_lags = pd.DataFrame({\n","    #saving city info for ease-of-use\n","    'cod_city': df_max_years['cod_city'],\n","    'x_coord': df_max_years['x_coord'],\n","    'y_coord': df_max_years['y_coord'],\n","    'year': df_max_years['Year'],\n","    'week': df_max_years['week'],\n","    #dengue cases\n","    #'cases_percent': y_dengue_percent,\n","    'cases': y_disease,\n","    'cases_lag_1': y_disease.shift(1),\n","    'cases_lag_2': y_disease.shift(2),\n","    'cases_lag_3': y_disease.shift(3),\n","    'cases_lag_4': y_disease.shift(4),\n","    'cases_lag_5': y_disease.shift(5)\n","})\n","\n","#Fill rows with missing data with zeros\n","df_lags = df_lags.fillna(0)"],"metadata":{"id":"DcsOMhBHJxWI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = 'C:/Users/your_path/' + disease + '/'\n","df_lags.to_csv(path + disease + '_weekly_first_5_lags.csv')"],"metadata":{"id":"iGrHNcAJeJye"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#DTW of the time series of cases"],"metadata":{"id":"gGtgJ4yQvwZB"}},{"cell_type":"code","source":["path = 'C:/Users/your_path/' + disease + '/'  #path to save the resulting files"],"metadata":{"id":"vMH1uWzq6gxw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cases_by_cities = df_lags[['cod_city', 'cases']]\n","\n","cases_by_cities = cases_by_cities.groupby(['cod_city'])['cases'].apply(lambda x: x.values.flatten())\n","cases_by_cities = pd.DataFrame(cases_by_cities.values.tolist(),index=cases_by_cities.index).add_prefix('obs_')"],"metadata":{"id":"MGF3cx1IUFzk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dtw_dist = dtw.distance_matrix_fast(cases_by_cities.to_numpy(dtype='float'))\n","df_dtw = pd.DataFrame(dtw_dist, index=cases_by_cities.index, columns=cases_by_cities.index)"],"metadata":{"id":"4Gwpqnzeba0i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_distances_dtw = pd.DataFrame()\n","\n","for city1 in list(df_dtw.index):\n","  first_city = []\n","  second_city = []\n","  dists = []\n","  for city2 in list(df_dtw.index):\n","    if (city1 != city2):\n","      first_city.append(city1)\n","      second_city.append(city2)\n","      dists.append(df_dtw[city1].loc[city2])\n","  data_distances = {'city_1': first_city, 'city_2': second_city, 'distance': dists}\n","  df_swp = pd.DataFrame(data_distances)\n","\n","  #We'll only save the closest 10 cities, to avoid a 5237^2 sized (immense) dataframe\n","  df_distances_dtw = pd.concat([df_distances_dtw, df_swp.sort_values(by='distance', ascending=True).head(10)])\n","\n","df_distances_dtw.to_csv(path + disease + '_distances_cases_dtw.csv')"],"metadata":{"id":"GaGx64zU1uWw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#DTW of PIB/GDP"],"metadata":{"id":"ZRCaIJcavLOj"}},{"cell_type":"code","source":["path = 'C:/Users/your_path/'\n","df_pib = pd.read_excel(path + 'pib_filtered_years.xls')\n","\n","df_pib['city'] = df_pib['city'].str.upper()\n","#-> Removing accents\n","cols = df_pib.select_dtypes(include=[object]).columns\n","df_pib[cols] = df_pib[cols].apply(lambda x: x.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8'))"],"metadata":{"id":"nJw_nQok_lsp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Using another dataframe to recover correct city codes for the GDP dataframe.\n","#-> Here I use the COVID19 one given its ample coverage of cities.\n","path = 'C:/Users/your_path/COVID19/'\n","df_dengue = pd.read_csv(path + 'covid_weekly_first_5_lags.csv')\n","\n","#Now we're going to compare this new database with the one containing only cities with continuous timeseries from 2020 to 2023.\n","df_pib_filtered = pd.DataFrame()\n","\n","for state in list(df_pib.state.unique()):\n","  state_data1 = df_pib.loc[df_pib['state'] == state]\n","  state_data2 = df_dengue.loc[df_dengue['state'] == state]\n","\n","  for city in list(state_data1.city.unique()):\n","    swp = state_data2.loc[state_data2['city'] == city]\n","    if(len(swp) != 0): #if the city is on the cluster list,\n","      swp_connections = state_data1.loc[state_data1['city'] == city]\n","      swp_connections['cod_city'] = np.repeat( swp['cod_city'].values[0], len(swp_connections) )\n","      df_pib_filtered = pd.concat([df_pib_filtered, swp_connections])\n","\n","#Now, filter by the cities we have for the disease being studied\n","df_pib_filtered = df_pib_filtered.loc[df_pib_filtered['cod_city'].isin(df_max_years.cod_city.unique())]"],"metadata":{"id":"Az1HkCAmCGdc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Applies DTW:"],"metadata":{"id":"7KCE_AtA9KN3"}},{"cell_type":"code","source":["pib_by_city = df_pib_filtered[['cod_city', 'pib_percapta_x1000rs']]\n","\n","pib_by_city = pib_by_city.groupby(['cod_city'])['pib_percapta_x1000rs'].apply(lambda x: x.values.flatten())\n","pib_by_city = pd.DataFrame(pib_by_city.values.tolist(),index=pib_by_city.index).add_prefix('year_')"],"metadata":{"id":"XlMSKQtFx2jG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dtw_dist = dtw.distance_matrix_fast(pib_by_city.values)\n","df_dtw = pd.DataFrame(dtw_dist, index=pib_by_city.index, columns=pib_by_city.index)"],"metadata":{"id":"RcXe1lxc3bUH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = 'C:/Users/your_path/'\n","\n","df_pib_dtw = pd.DataFrame()\n","\n","for city1 in list(df_dtw.index):\n","  first_city = []\n","  second_city = []\n","  dists = []\n","  for city2 in list(df_dtw.index):\n","    if (city1 != city2):\n","      first_city.append(city1)\n","      second_city.append(city2)\n","      dists.append(df_dtw[city1].loc[city2])\n","  data_distances = {'city_1': first_city, 'city_2': second_city, 'distance': dists}\n","  df_swp = pd.DataFrame(data_distances)\n","\n","  #We'll only save the closest 10 cities, to avoid a 1772^2 sized (immense) dataframe\n","  df_pib_dtw = pd.concat([df_pib_dtw, df_swp.sort_values(by='distance', ascending=True).head(10)])\n","\n","df_pib_dtw.to_csv(path + disease + '_pib_dtw.csv')"],"metadata":{"id":"tCIeHudx3bUJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Cities to be analyzed for all methods\n","cities_to_use = pd.DataFrame(data=df_pib_filtered.cod_city.unique(), columns=['cities_to_keep'])\n","cities_to_use.to_csv(path + disease + '_cities_to_keep.csv')"],"metadata":{"id":"fbbewG_2-jE0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generate the distances dataframe"],"metadata":{"id":"99VryJhhcHkN"}},{"cell_type":"markdown","source":["Depending on the amount of cities being investigated in a certain dataframe, this block can take a long while to finish its execution, as it does not use a parallel structure."],"metadata":{"id":"AGYDoItg-7jZ"}},{"cell_type":"code","source":["path = 'C:/Users/your_path/'"],"metadata":{"id":"eP4VHa_uYhMm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n = 1\n","df_distances = pd.DataFrame()\n","data_to_use = df_lags\n","\n","for city in list(data_to_use['cod_city'].unique()):\n","  print('City', n, 'of', len(list(data_to_use['cod_city'].unique())))\n","  dist = []\n","  city1 = []\n","  city2 = []\n","\n","  cities_to_compare = list(data_to_use['cod_city'].unique())\n","  cities_to_compare.remove(city)\n","  x1 = data_to_use.loc[data_to_use['cod_city'] == city]['x_coord'].values[0]\n","  y1 = data_to_use.loc[data_to_use['cod_city'] == city]['y_coord'].values[0]\n","  for other_city in cities_to_compare:\n","    city1.append(city)\n","    city2.append(other_city)\n","    x2 = data_to_use.loc[data_to_use['cod_city'] == other_city]['x_coord'].values[0]\n","    y2 = data_to_use.loc[data_to_use['cod_city'] == other_city]['y_coord'].values[0]\n","    dist.append( np.sqrt( np.power( (x1-x2) ,2) + np.power( (y1-y2) ,2) ) )\n","\n","  data_distances = {'city_1': city1, 'city_2': city2, 'distance': dist}\n","  df_swp = pd.DataFrame(data_distances)\n","\n","  #We'll only save the closest 10 cities\n","  df_distances = pd.concat([df_distances, df_swp.sort_values(by='distance', ascending=True).head(10)])\n","  n = n+1\n","\n","df_distances.to_csv(path + disease + '_geo_distances_cities.csv')"],"metadata":{"id":"F5fe80drcLUW"},"execution_count":null,"outputs":[]}]}