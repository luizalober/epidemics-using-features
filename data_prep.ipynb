{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import calendar\n",
        "import time\n",
        "import glob\n",
        "import os\n",
        "\n",
        "#Disable the 'SettingWithCopyWarning' when adding new columns to an existing dataframe\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "\n",
        "#Data cleaning\n",
        "from functools import reduce\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "#Ignore warnings about the DataFrameGroupBy() method\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "#For the graphics\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "#Feature selection\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "#Modeling\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import ParameterGrid   #to implement manual GridSearch\n",
        "\n",
        "#Dynamic time warping: grouping\n",
        "from dtaidistance import dtw\n",
        "from dtaidistance import dtw_ndim"
      ],
      "metadata": {
        "id": "37nDIHg_VxzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data"
      ],
      "metadata": {
        "id": "c2xMJI-pbg2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dengue or Zika: Run this first"
      ],
      "metadata": {
        "id": "XlQ-Kz7bbbJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "disease = 'zika'\n",
        "path = \"your_path/\""
      ],
      "metadata": {
        "id": "OTSiOTCcjFep"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read all data\n",
        "#-> IMPORTANT: I'm using a naming convention of choice here for the data coming from the \"data_cleaning\" notebooks.\n",
        "#-> If you're doing it differently, remember to chance it below.\n",
        "df = pd.DataFrame()\n",
        "\n",
        "if disease == 'dengue':\n",
        "  init_year = 2014\n",
        "  end_year = 2023\n",
        "  columns_filter = 9\n",
        "\n",
        "if disease == 'zika':\n",
        "  init_year = 2016\n",
        "  end_year = 2024\n",
        "  columns_filter = 3\n",
        "\n",
        "for year in range(init_year, end_year, 1):   #2014-2022 to weekly data.\n",
        "  #Open a table for each year that we have weekly dengue data.\n",
        "  df_year = pd.read_csv(str(path + '/SINAN_' + disease + '_weekly_' + str(year) + '.csv'))\n",
        "  df_year.drop(columns='Unnamed: 0', inplace=True)\n",
        "\n",
        "  full_city_to_dengue_list = []   #rather inefficient vector, but it works.\n",
        "\n",
        "  #These loops in UFs and cities are here to guarantee that I'm selecting unique\n",
        "  #-> cities in each state, as identical city names is a common occurrence in Brazil.\n",
        "  for city_codes in list(df_year['cod_city'].unique()):\n",
        "    city_data = df_year.loc[df_year['cod_city'] == city_codes].values[0][columns_filter:]\n",
        "    for value in range(0, len(city_data), 1):\n",
        "      full_city_to_dengue_list.append(city_data[value])   #saves each value in the selected row in this array\n",
        "\n",
        "  df_year.drop(columns=df_year.columns[9:], inplace=True)   #no need to keep the dengue cases count anymore\n",
        "\n",
        "  df_year_mod = pd.DataFrame(np.repeat(df_year.values, 52, axis=0))   #expand the dataframe to add the year and week columns later\n",
        "  df_year_mod.columns = df_year.columns\n",
        "\n",
        "  year_list = np.full(len(df_year_mod), fill_value=year)\n",
        "  week_list = np.tile(list(range(1,53)), ( int(len(full_city_to_dengue_list)/52) ) )\n",
        "\n",
        "  #Adds the year, week and cases columns to the new dataframe\n",
        "  df_year_mod['Year'] = year_list\n",
        "  df_year_mod['week'] = week_list\n",
        "  df_year_mod['cases'] = full_city_to_dengue_list\n",
        "\n",
        "  #Concatenate the previous result with what was already done\n",
        "  df = pd.concat([df, df_year_mod])\n",
        "\n",
        "#only one language in the dataframe, please.\n",
        "df = df.rename(columns={'UF': 'state', 'Município de notificação': 'city'})\n",
        "\n",
        "#Arrange by state and year\n",
        "if disease == 'dengue':\n",
        "  df.sort_values(by=['state', 'city', 'Year'], ascending=[True, True, True], inplace=True)\n",
        "if disease == 'zika':\n",
        "  df.sort_values(by=['cod_city', 'Year'], ascending=[True, True], inplace=True)\n",
        "df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "#Remove region names and reorganize the dataframe to the same order used in filtered data\n",
        "if disease == 'dengue':\n",
        "  df.drop(columns=['name_rgi', 'name_rgint'], inplace=True)\n",
        "\n",
        "if disease == 'dengue':\n",
        "  df = df[['state', 'cod_rgi', 'cod_rgint', 'cod_city', 'city', 'x_coord', 'y_coord', 'week', 'Year', 'cases']]\n",
        "if disease == 'zika':\n",
        "  df = df[['cod_city','x_coord', 'y_coord', 'week', 'Year', 'cases']]"
      ],
      "metadata": {
        "id": "Li8PXfViChOI"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#COVID-19 or Influenza: Run this first"
      ],
      "metadata": {
        "id": "gzw6FoVRbtF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "disease = 'covid'\n",
        "#path = 'your_path/' + disease + '/'\n",
        "path = '/media/llober/Time/Doutorado/Trabalhos/Projeto-dengue/test_verif/'\n",
        "\n",
        "#packs all yearly files in a single data frame.\n",
        "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
        "df = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)\n",
        "df.drop(columns='Unnamed: 0', inplace=True)"
      ],
      "metadata": {
        "id": "fyiyzbO4RkY2"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filtering the data by years"
      ],
      "metadata": {
        "id": "vV-yi296rQtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#save the cities and the amount of years each one contains in its time series.\n",
        "test_years = []\n",
        "test_cities = []\n",
        "removed_cities = 0\n",
        "removed_cities_names = []\n",
        "\n",
        "df_filled = df.copy()\n",
        "df_filled = df_filled.fillna(0)\n",
        "city_counter = 0\n",
        "\n",
        "for city in list(df_filled.cod_city.unique()):\n",
        "  #print('City', city_counter+1, 'of', len(list(df_filled.cod_city.unique())))\n",
        "  swp = df_filled.loc[ (df_filled['cod_city'] == city) ]\n",
        "  years = list( swp.loc[swp['cod_city'] == city].Year.unique() )\n",
        "\n",
        "  test_cities.append(city)\n",
        "  test_years.append(len(years))\n",
        "\n",
        "  city_counter = city_counter+1\n",
        "\n",
        "\n",
        "cities_lenght_data = {'city': test_cities, 'n_years': test_years}\n",
        "cities_lenght = pd.DataFrame(data=cities_lenght_data)\n",
        "cities_lenght.sort_values(by='n_years', ascending=False, inplace=True)\n",
        "\n",
        "\n",
        "df_max_years = pd.DataFrame()\n",
        "cities = cities_lenght.loc[cities_lenght['n_years'] == max(cities_lenght.n_years.unique())]\n",
        "\n",
        "for city in list(cities.city.unique()):\n",
        "  swp = df_filled.loc[df_filled['cod_city'] == city]\n",
        "  if (len(swp) !=0):\n",
        "    df_max_years = pd.concat([df_max_years, swp])"
      ],
      "metadata": {
        "id": "kWpDKj3y3Njm"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding lags"
      ],
      "metadata": {
        "id": "D0pgYEPw4Dua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_disease = df_max_years['cases']\n",
        "\n",
        "df_lags = pd.DataFrame({\n",
        "    #saving city info for ease-of-use\n",
        "    'cod_city': df_max_years['cod_city'],\n",
        "    'x_coord': df_max_years['x_coord'],\n",
        "    'y_coord': df_max_years['y_coord'],\n",
        "    'year': df_max_years['Year'],\n",
        "    'week': df_max_years['week'],\n",
        "    #dengue cases\n",
        "    #'cases_percent': y_dengue_percent,\n",
        "    'cases': y_disease,\n",
        "    'cases_lag_1': y_disease.shift(1),\n",
        "    'cases_lag_2': y_disease.shift(2),\n",
        "    'cases_lag_3': y_disease.shift(3),\n",
        "    'cases_lag_4': y_disease.shift(4),\n",
        "    'cases_lag_5': y_disease.shift(5)\n",
        "})\n",
        "\n",
        "#Fill rows with missing data with zeros\n",
        "df_lags = df_lags.fillna(0)"
      ],
      "metadata": {
        "id": "DcsOMhBHJxWI"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#path = 'C:/Users/your_path/' + disease + '/'\n",
        "#df_lags.to_csv(path + disease + '_weekly_first_5_lags.csv')\n",
        "df_lags.to_csv('covid_lags_test.csv')"
      ],
      "metadata": {
        "id": "iGrHNcAJeJye"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DTW of the time series of cases"
      ],
      "metadata": {
        "id": "gGtgJ4yQvwZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'your_path/' + disease + '/'  #path to save the resulting files"
      ],
      "metadata": {
        "id": "vMH1uWzq6gxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cases_by_cities = df_lags[['cod_city', 'cases']]\n",
        "\n",
        "cases_by_cities = cases_by_cities.groupby(['cod_city'])['cases'].apply(lambda x: x.values.flatten())\n",
        "cases_by_cities = pd.DataFrame(cases_by_cities.values.tolist(),index=cases_by_cities.index).add_prefix('obs_')"
      ],
      "metadata": {
        "id": "MGF3cx1IUFzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtw_dist = dtw.distance_matrix_fast(cases_by_cities.to_numpy(dtype='float'))\n",
        "df_dtw = pd.DataFrame(dtw_dist, index=cases_by_cities.index, columns=cases_by_cities.index)"
      ],
      "metadata": {
        "id": "4Gwpqnzeba0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_distances_dtw = pd.DataFrame()\n",
        "\n",
        "for city1 in list(df_dtw.index):\n",
        "  first_city = []\n",
        "  second_city = []\n",
        "  dists = []\n",
        "  for city2 in list(df_dtw.index):\n",
        "    if (city1 != city2):\n",
        "      first_city.append(city1)\n",
        "      second_city.append(city2)\n",
        "      dists.append(df_dtw[city1].loc[city2])\n",
        "  data_distances = {'city_1': first_city, 'city_2': second_city, 'distance': dists}\n",
        "  df_swp = pd.DataFrame(data_distances)\n",
        "\n",
        "  #We'll only save the closest 10 cities, to avoid a 5237^2 sized (immense) dataframe\n",
        "  df_distances_dtw = pd.concat([df_distances_dtw, df_swp.sort_values(by='distance', ascending=True).head(10)])\n",
        "\n",
        "df_distances_dtw.to_csv(path + disease + '_distances_cases_dtw.csv')"
      ],
      "metadata": {
        "id": "GaGx64zU1uWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DTW of PIB/GDP"
      ],
      "metadata": {
        "id": "ZRCaIJcavLOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'your_path/'\n",
        "df_pib = pd.read_excel(path + 'pib_filtered_years.xls')\n",
        "\n",
        "df_pib['city'] = df_pib['city'].str.upper()\n",
        "#-> Removing accents\n",
        "cols = df_pib.select_dtypes(include=[object]).columns\n",
        "df_pib[cols] = df_pib[cols].apply(lambda x: x.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8'))"
      ],
      "metadata": {
        "id": "nJw_nQok_lsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using another dataframe to recover correct city codes for the GDP dataframe.\n",
        "#-> Here I use the COVID19 one given its ample coverage of cities.\n",
        "path = 'your_path/COVID19/'\n",
        "df_dengue = pd.read_csv(path + 'covid_weekly_first_5_lags.csv')\n",
        "\n",
        "#Now we're going to compare this new database with the one containing only cities with continuous timeseries from 2020 to 2023.\n",
        "df_pib_filtered = pd.DataFrame()\n",
        "\n",
        "for state in list(df_pib.state.unique()):\n",
        "  state_data1 = df_pib.loc[df_pib['state'] == state]\n",
        "  state_data2 = df_dengue.loc[df_dengue['state'] == state]\n",
        "\n",
        "  for city in list(state_data1.city.unique()):\n",
        "    swp = state_data2.loc[state_data2['city'] == city]\n",
        "    if(len(swp) != 0): #if the city is on the cluster list,\n",
        "      swp_connections = state_data1.loc[state_data1['city'] == city]\n",
        "      swp_connections['cod_city'] = np.repeat( swp['cod_city'].values[0], len(swp_connections) )\n",
        "      df_pib_filtered = pd.concat([df_pib_filtered, swp_connections])\n",
        "\n",
        "#Now, filter by the cities we have for the disease being studied\n",
        "df_pib_filtered = df_pib_filtered.loc[df_pib_filtered['cod_city'].isin(df_max_years.cod_city.unique())]"
      ],
      "metadata": {
        "id": "Az1HkCAmCGdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applies DTW:"
      ],
      "metadata": {
        "id": "7KCE_AtA9KN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pib_by_city = df_pib_filtered[['cod_city', 'pib_percapta_x1000rs']]\n",
        "\n",
        "pib_by_city = pib_by_city.groupby(['cod_city'])['pib_percapta_x1000rs'].apply(lambda x: x.values.flatten())\n",
        "pib_by_city = pd.DataFrame(pib_by_city.values.tolist(),index=pib_by_city.index).add_prefix('year_')"
      ],
      "metadata": {
        "id": "XlMSKQtFx2jG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtw_dist = dtw.distance_matrix_fast(pib_by_city.values)\n",
        "df_dtw = pd.DataFrame(dtw_dist, index=pib_by_city.index, columns=pib_by_city.index)"
      ],
      "metadata": {
        "id": "RcXe1lxc3bUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'your_path/'\n",
        "\n",
        "df_pib_dtw = pd.DataFrame()\n",
        "\n",
        "for city1 in list(df_dtw.index):\n",
        "  first_city = []\n",
        "  second_city = []\n",
        "  dists = []\n",
        "  for city2 in list(df_dtw.index):\n",
        "    if (city1 != city2):\n",
        "      first_city.append(city1)\n",
        "      second_city.append(city2)\n",
        "      dists.append(df_dtw[city1].loc[city2])\n",
        "  data_distances = {'city_1': first_city, 'city_2': second_city, 'distance': dists}\n",
        "  df_swp = pd.DataFrame(data_distances)\n",
        "\n",
        "  #We'll only save the closest 10 cities, to avoid a 1772^2 sized (immense) dataframe\n",
        "  df_pib_dtw = pd.concat([df_pib_dtw, df_swp.sort_values(by='distance', ascending=True).head(10)])\n",
        "\n",
        "df_pib_dtw.to_csv(path + disease + '_pib_dtw.csv')"
      ],
      "metadata": {
        "id": "tCIeHudx3bUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cities to be analyzed for all methods\n",
        "cities_to_use = pd.DataFrame(data=df_pib_filtered.cod_city.unique(), columns=['cities_to_keep'])\n",
        "cities_to_use.to_csv(path + disease + '_cities_to_keep.csv')"
      ],
      "metadata": {
        "id": "fbbewG_2-jE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate the distances dataframe"
      ],
      "metadata": {
        "id": "99VryJhhcHkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on the amount of cities being investigated in a certain dataframe, this block can take a long while to finish its execution, as it does not use a parallel structure."
      ],
      "metadata": {
        "id": "AGYDoItg-7jZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'your_path/'"
      ],
      "metadata": {
        "id": "eP4VHa_uYhMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1\n",
        "df_distances = pd.DataFrame()\n",
        "data_to_use = df_lags\n",
        "\n",
        "for city in list(data_to_use['cod_city'].unique()):\n",
        "  print('City', n, 'of', len(list(data_to_use['cod_city'].unique())))\n",
        "  dist = []\n",
        "  city1 = []\n",
        "  city2 = []\n",
        "\n",
        "  cities_to_compare = list(data_to_use['cod_city'].unique())\n",
        "  cities_to_compare.remove(city)\n",
        "  x1 = data_to_use.loc[data_to_use['cod_city'] == city]['x_coord'].values[0]\n",
        "  y1 = data_to_use.loc[data_to_use['cod_city'] == city]['y_coord'].values[0]\n",
        "  for other_city in cities_to_compare:\n",
        "    city1.append(city)\n",
        "    city2.append(other_city)\n",
        "    x2 = data_to_use.loc[data_to_use['cod_city'] == other_city]['x_coord'].values[0]\n",
        "    y2 = data_to_use.loc[data_to_use['cod_city'] == other_city]['y_coord'].values[0]\n",
        "    dist.append( np.sqrt( np.power( (x1-x2) ,2) + np.power( (y1-y2) ,2) ) )\n",
        "\n",
        "  data_distances = {'city_1': city1, 'city_2': city2, 'distance': dist}\n",
        "  df_swp = pd.DataFrame(data_distances)\n",
        "\n",
        "  #We'll only save the closest 10 cities\n",
        "  df_distances = pd.concat([df_distances, df_swp.sort_values(by='distance', ascending=True).head(10)])\n",
        "  n = n+1\n",
        "\n",
        "df_distances.to_csv(path + disease + '_geo_distances_cities.csv')"
      ],
      "metadata": {
        "id": "F5fe80drcLUW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}