{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["7hP9zTxCzwqw","zqdlvn8VcBBL","bF5MKoe_lK3-","ZYwjwNuzHFUG","c_2Vj4ruHGHI"],"toc_visible":true,"authorship_tag":"ABX9TyP2BmFy2N9hNihcCuTPOczX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 0) Packages and functions"],"metadata":{"id":"7hP9zTxCzwqw"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import calendar\n","import time\n","import glob\n","import os\n","import scipy\n","\n","#Disable the 'SettingWithCopyWarning' when adding new columns to an existing dataframe\n","pd.options.mode.chained_assignment = None  # default='warn'\n","\n","#Data cleaning\n","from functools import reduce\n","import unicodedata\n","import re\n","\n","#Ignore warnings about the DataFrameGroupBy() method\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","\n","#For the graphics\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import matplotlib.dates as mdates\n","import matplotlib.patches as mpatches\n","\n","#Feature selection\n","from sklearn.inspection import permutation_importance\n","\n","#Modeling\n","from sklearn.model_selection import cross_validate\n","from sklearn.model_selection import TimeSeriesSplit\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import ParameterGrid   #to implement manual GridSearch\n","#-> RF\n","from sklearn.ensemble import RandomForestRegressor\n","#-> GBR\n","from sklearn.ensemble import GradientBoostingRegressor\n","#-> CatBoost\n","#!pip install catboost\n","#import catboost\n","#-> LightGBM\n","#!pip install LightGBM\n","#import lightgbm\n","#-> XGBoost\n","#!pip install XGBoost\n","import xgboost\n","\n","#Metrics\n","from sklearn.metrics import mean_absolute_error, r2_score\n","from sklearn.metrics import make_scorer   #needed to use MASE on the cross validation stages\n","\n","#Dynamic time warping: grouping\n","from dtaidistance import dtw\n","from dtaidistance import dtw_ndim"],"metadata":{"id":"37nDIHg_VxzO","executionInfo":{"status":"ok","timestamp":1711999755360,"user_tz":0,"elapsed":2450,"user":{"displayName":"Luiza Lober de Souza Piva","userId":"13136650427747089256"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["def describe(df, data_description, anomaly_any, stats=['skew', 'median']):\n","  '''\n","  Custom describe function. Adds more statistics, which were used to complement the\n","  mean and best characterize the resutls, to the usual pd.describe()\n","\n","  df: Dataframe containing the results to be analysed\n","  data_description: string used during simulations. Used to filter each time series\n","  -> selection criteria (such as GDP) from the complete results dataframe.\n","  anomaly_any: binary. 0 is used to filter results, while 1 shows data for all tested cities.\n","  stats: skewness and median as default, but could accept more entries (see statistics implemented in the Pandas package).\n","  '''\n","\n","  #Filters by data description\n","  df = df.loc[df['data_description'] == data_description]\n","  if (anomaly_any == 0):    #selects data for cities without anomalies\n","    df = df.loc[df['anomaly_any'] == anomaly_any]\n","  #Define custom percentiles. My argument here is that we still get results better than the naÃ¯ve model on up to 85% of all data\n","  d = df.describe(percentiles=[.25, .5, .85])\n","  d.drop(columns=['cod_city', 'exec_time'], inplace=True)   #show only MASE and MAE\n","  aux = df.reindex(d.columns, axis = 1).agg(stats)\n","  d = pd.concat([d, aux])\n","\n","  return d"],"metadata":{"id":"c85sxgSf4mCh","executionInfo":{"status":"ok","timestamp":1711999755376,"user_tz":0,"elapsed":16,"user":{"displayName":"Luiza Lober de Souza Piva","userId":"13136650427747089256"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Accuracy evaluation"],"metadata":{"id":"XoZyb6VHDLmZ"}},{"cell_type":"code","source":["def mean_absolute_scaled_error(y_true, y_pred, y_train):\n","    \"\"\"\n","    Computes the MEAN-ABSOLUTE SCALED ERROR forecast error for univariate time series prediction.\n","\n","    See \"Another look at measures of forecast accuracy\", Rob J Hyndman\n","\n","    parameters:\n","        y_train: the series used to train the model, 1d numpy array\n","        y_true: the test series to predict, 1d numpy array or float\n","        y_pred: the prediction of y_true, 1d numpy array (same size as y_true) or float\n","        absolute: \"squares\" to use sum of squares and root the result, \"absolute\" to use absolute values.\n","\n","    \"\"\"\n","    n = y_train.shape[0]\n","    d = np.abs( np.diff( y_train) ).sum()/(n-1)\n","\n","    errors = mean_absolute_error(y_true, y_pred)\n","    return errors.mean()/d"],"metadata":{"id":"LqiwgUjqAx66","executionInfo":{"status":"ok","timestamp":1711999755376,"user_tz":0,"elapsed":0,"user":{"displayName":"Luiza Lober de Souza Piva","userId":"13136650427747089256"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def best_model_GridSearchCV(param_grid, X_train, y_train, splits, base_estimator):\n","  start_run = time.time()\n","  ts_cv = TimeSeriesSplit(n_splits=splits)\n","  mase_scorer = make_scorer(mean_absolute_scaled_error, greater_is_better=False, y_train=y_train)\n","  scorer = {'mae': 'neg_mean_absolute_error', 'mase': mase_scorer}\n","\n","  sh = GridSearchCV(base_estimator, param_grid, scoring=scorer, refit='mase', cv=splits, n_jobs=-1).fit(X_train, y_train)\n","  best_model = sh.best_estimator_\n","  best_params = sh.best_params_\n","\n","  cv_results = cross_validate(\n","    best_model,\n","    X_train,\n","    y_train,\n","    cv=ts_cv,\n","    scoring=scorer,\n","    return_train_score=True\n","  )\n","  #Train\n","  train_mae = -cv_results[\"train_mae\"]\n","  train_mase = -cv_results[\"train_mase\"]\n","\n","  #Val\n","  val_mae = -cv_results[\"test_mae\"]\n","  val_mase = -cv_results[\"test_mase\"]\n","\n","  end_run = time.time()\n","  runtime = round( (end_run - start_run), 2)\n","\n","  return best_model, best_params, runtime, train_mae.mean(), train_mase.mean(), val_mae.mean(), val_mase.mean()"],"metadata":{"id":"TfjjS7bC0Zu4","executionInfo":{"status":"ok","timestamp":1711999755376,"user_tz":0,"elapsed":0,"user":{"displayName":"Luiza Lober de Souza Piva","userId":"13136650427747089256"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Anomaly detection"],"metadata":{"id":"bwlroaqzDe5U"}},{"cell_type":"code","source":["def detect_anomalies_test(data, name_pred, test_size=0.2):\n","  df_city = data.loc[data['cod_city'] == name_pred]\n","  y = df_city[df_city.columns[7]]\n","  df_city = df_city.drop(columns=list(df_city.columns[:7]))\n","  X = df_city[df_city.columns[1:]]    #Not really necessary to the evaluation, but the split needs it\n","\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, shuffle=False)\n","\n","  # calculate mean\n","  mean = np.mean(y_train.values)\n","  # calculate standard deviation\n","  sd = np.std(y_train.values)\n","  # determine a threshold\n","  threshold = 4   #4 sigma\n","\n","  #Count the number of zeros in the train set\n","  zeros = (y_train.values == 0).astype(int).sum() / len(y_train)\n","\n","  #Initialize response variables\n","  z_score = []\n","  anomaly_class = 0\n","\n","  # detect outlier\n","  for i in y_test.values:\n","    z = (i-mean)/sd # calculate z-score\n","    if abs(z) > threshold:  # identify outliers\n","      anomaly_class = 1   #it's enough to know that there's at least one outlier in the test set\n","      z_score.append(z)   #add all z scores to a list\n","\n","  if (len(z_score) == 0):\n","    z_score = 0\n","    return anomaly_class, z_score, zeros\n","  else:\n","    return anomaly_class, np.mean(z_score), round(zeros, 3)"],"metadata":{"id":"pWsUFKCsaCov","executionInfo":{"status":"ok","timestamp":1711999755393,"user_tz":0,"elapsed":17,"user":{"displayName":"Luiza Lober de Souza Piva","userId":"13136650427747089256"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Train-test split"],"metadata":{"id":"VVkeXrgJDiZB"}},{"cell_type":"code","source":["def get_train_test_extended(disease, data, distances_data, cod_city, agg_type, number_of_cities, scaled=False):\n","  '''\n","  distances_data: Pandas Dataframe. Assumes the columns 'city_1', city_2' and 'distance', in this order.\n","  agg_type: 'topk' or 'topk_avg'\n","  '''\n","  #Filter the data according to the chosen division - legacy (left here for debugging purposes)\n","  topk = list(distances_data.loc[distances_data['city_1'] == cod_city]['city_2'].values)\n","  topk = topk[:number_of_cities]\n","\n","  data_filtered = data.loc[data['cod_city'].isin(topk)]\n","\n","  #Get the cities contained in the division\n","  cities = list(data_filtered.cod_city.unique())\n","  city_counter = 1\n","\n","  #Select the city being studied\n","  if (disease == 'covid'):\n","    index_cases = 7\n","  if ((disease == 'zika') or (disease == 'influenza')):\n","    index_cases = 5\n","  if (disease == 'dengue'):\n","    index_cases = 9\n","\n","  X_extra = pd.DataFrame()\n","  for city in cities:\n","    swp = data_filtered.loc[data_filtered['cod_city'] == city]\n","    swp = swp.reset_index()\n","    swp.drop(columns='index', inplace=True)\n","\n","    #rename the columns to make each data column from the cities unique.\n","    name_columns = list(swp.columns)\n","    renamed_columns = ['nb' + str(city_counter) + '_' + category for category in name_columns]\n","    swp.columns = renamed_columns\n","    swp = swp[swp.columns[index_cases]]\n","    X_extra = pd.concat([X_extra, swp], axis=1)\n","    city_counter = city_counter + 1\n","\n","  df_city = data.loc[data['cod_city'] == cod_city]\n","  y = df_city[df_city.columns[index_cases]]   #this split respects the structure of the dataset containing diseases cases after using the data prep notebook.\n","  df_city = df_city.drop(columns=list(df_city.columns[:index_cases]))\n","  X = df_city[df_city.columns[1:]]    #lags for the city being studied, features\n","\n","  if (agg_type == 'topk'):\n","    X = X.reset_index()\n","    X.drop(columns='index', inplace=True)\n","    X = pd.concat([X, X_extra], axis=1)\n","\n","  if (agg_type == 'topk_avg'):\n","    X_swp = pd.DataFrame()\n","    for n in range(1, len(cities)+1):\n","      X_columns_city = list(X_extra.filter(regex=str('nb' + str(n) + '_')))\n","      swp2 = X_extra[X_columns_city].mean(axis=1)\n","      swp2 = swp2.to_frame(name=str('nb' + str(n) + '_lag_avg'))\n","      X_swp = pd.concat([X_swp, swp2], axis=1)\n","\n","    X = X.reset_index()\n","    X.drop(columns='index', inplace=True)\n","    X = pd.concat([X, X_swp], axis=1)\n","\n","  #After assembling the dataframe, apply the train/test split.\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n","                                                      random_state=42, shuffle=False)   #1/15: the test size will be made of 1 year of observations\n","  if(scaled == True):\n","    scaler_series = preprocessing.MinMaxScaler()\n","    X_train = scaler_series.fit_transform(X_train)\n","    X_test = scaler_series.transform(X_test)\n","    return X_train, X_test, y_train, y_test, scaler_series\n","  else:\n","    return X_train, X_test, y_train, y_test"],"metadata":{"id":"Pp3CGll2Cv7Y","executionInfo":{"status":"ok","timestamp":1712000227436,"user_tz":0,"elapsed":0,"user":{"displayName":"Luiza Lober de Souza Piva","userId":"13136650427747089256"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["def get_train_test_single(disease, data, cod_city, scaled=False):\n","  '''\n","  This code selects the data of the city (cod_city) being studied and converts it\n","  to a darTS input.\n","  '''\n","\n","  #Select the city being studied\n","  df_city = data.loc[data['cod_city'] == cod_city]\n","  df_city.reset_index(inplace=True, drop=True)\n","\n","  #Select the city being studied\n","  if (disease == 'zika'):\n","    index_cases = 9\n","  if ((disease == 'zika') or (disease == 'influenza')):\n","    index_cases = 7\n","  if (disease == 'dengue'):\n","    index_cases = 11\n","  X = df_city[df_city.columns[index_cases+1:]]  #this split respects the structure of the dataset containing diseases cases after using the data prep notebook.\n","  y = df_city[df_city.columns[index_cases]]\n","\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n","                                                      random_state=42, shuffle=False)   #1/15: the test size will be made of 1 year (2022) of observations\n","  if(scaled):\n","    scaler_series = preprocessing.MinMaxScaler()\n","    X_train = scaler_series.fit_transform(X_train)\n","    X_test = scaler_series.transform(X_test)\n","    return X_train, X_test, y_train, y_test, scaler_series\n","  else:\n","    return X_train, X_test, y_train, y_test"],"metadata":{"id":"tY9C2qk6rQiY","executionInfo":{"status":"ok","timestamp":1711999755410,"user_tz":0,"elapsed":0,"user":{"displayName":"Luiza Lober de Souza Piva","userId":"13136650427747089256"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Function to execute direct, one-to-one, predictions"],"metadata":{"id":"CzFT8bdnDt5f"}},{"cell_type":"code","source":["def run_model(disease, model, data, distances_data, data_description, pred_type, name_pred,\n","              number_of_cities, scaled=True):\n","  '''\n","  model: string to determine which model to use for regression. Options: RF, GBR, CatBoost, LightGBM, XGBoost\n","  X and y: features and target, respectively, to be used, which should be generated with closest_cities().\n","  data_description: can be a loose string. I use it latter to aggregate results on graphics.\n","  name_pred: string describing the city or region being analysed\n","  scaled: boolean. Wheter to use scaled train/test sets or not.\n","  '''\n","  #Model selection\n","  if (model == 'RF'):\n","    grid = {'n_estimators': [25,50,100,150,200], 'max_depth': [2,4,None]}\n","    estimator = RandomForestRegressor(random_state=42)\n","\n","  if (model == 'GBR'):\n","    grid = {'n_estimators': [25,50,100,150,200], 'max_depth': [2,4,None]}\n","    estimator = GradientBoostingRegressor(random_state=42)\n","\n","  if (model == 'CatBoost'):\n","    grid = {'iterations': [25,50,100,150,200], 'depth': [2,4,None]}\n","    estimator = catboost.CatBoostRegressor(random_state=42, silent=True)   #silent: This model will have a lot to say otherwise.\n","\n","  if (model == 'LightGBM'):\n","    grid = {'n_estimators': [50,100,150], 'max_depth': [2,4,None]}\n","    estimator = lightgbm.LGBMRegressor(random_state=42)\n","\n","  if (model == 'XGBoost'):\n","    grid = {'n_estimators': [25,50,100,150,200], 'learning_rate': [0.001, 0.005, 0.01],\n","            'max_depth': [2,4,None]}\n","    estimator = xgboost.XGBRegressor(random_state=42)\n","\n","  #Splitting the datasets\n","  if (pred_type == 'single'):\n","    X_train, X_test, y_train, y_test, _ = get_train_test_single(disease, data, name_pred, scaled=scaled)\n","  if ((pred_type == 'topk') or (pred_type == 'topk_avg')):\n","    X_train, X_test, y_train, y_test, _ = get_train_test_extended(disease, data, distances_data, name_pred, pred_type, number_of_cities, scaled=scaled)\n","\n","  #[Train and val using CV] Model predictions on the best parameter combination found by exaustive GridSearch + CV\n","  model_CV, params, runtime, train_mae, train_mase, val_mae, val_mase = best_model_GridSearchCV(\n","      param_grid=grid, X_train=X_train, y_train=y_train, splits=4, base_estimator=estimator)\n","\n","  par_names = [list(params.keys())]\n","  par_values = [list(params.values())]\n","\n","  #[Test set, hold-out]\n","  model_CV.fit(X_train, y_train)\n","  y_pred_test = model_CV.predict(X_test)\n","  test_mae = mean_absolute_error(y_test, y_pred_test)\n","  test_mase = mean_absolute_scaled_error(y_test, y_pred_test, y_train)\n","\n","  #Detect anomalies in the test set\n","  anomaly_class, mean_z_score, _ = detect_anomalies_test(data, name_pred, test_size=0.2)\n","\n","  res = {'model': model, 'pred_for': pred_type, 'cod_city': name_pred,\n","         'data_description': data_description, 'par_names': par_names, 'par_values': par_values,\n","         'scaled?': scaled, 'exec_time': runtime, 'mae_train': train_mae,\n","         'mase_train': train_mase, 'mae_val': val_mae, 'mase_val': val_mase,\n","         'mae_test': test_mae, 'mase_test': test_mase, 'anomaly_any':anomaly_class,\n","         'z_score': mean_z_score}\n","\n","  df_res = pd.DataFrame(res, index=[0])\n","  print(model, 'runtime:', runtime, 's.')\n","\n","  return df_res"],"metadata":{"id":"cQ4IRL6WztiP","executionInfo":{"status":"ok","timestamp":1711999755410,"user_tz":0,"elapsed":0,"user":{"displayName":"Luiza Lober de Souza Piva","userId":"13136650427747089256"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def display_model(model, data, distances_data, data_description, pred_type, name_pred,\n","              number_of_cities, scaled=True):\n","  '''\n","  Almost the same function as the previous one. Instead of displaying accuracies, here\n","  the results are the predicted time series. Useful for generating visualizations.\n","  '''\n","  #Model selection\n","  if (model == 'RF'):\n","    grid = {'n_estimators': [25,50,100,150,200], 'max_depth': [2,4,None]}\n","    estimator = RandomForestRegressor(random_state=42)\n","\n","  if (model == 'GBR'):\n","    grid = {'n_estimators': [25,50,100,150,200], 'max_depth': [2,4,None]}\n","    estimator = GradientBoostingRegressor(random_state=42)\n","\n","  if (model == 'CatBoost'):\n","    grid = {'iterations': [25,50,100,150,200], 'depth': [2,4,None]}\n","    estimator = catboost.CatBoostRegressor(random_state=42, silent=True)   #silent: This model will have a lot to say otherwise.\n","\n","  if (model == 'LightGBM'):\n","    grid = {'n_estimators': [50,100,150], 'max_depth': [2,4,None]}\n","    estimator = lightgbm.LGBMRegressor(random_state=42)\n","\n","  if (model == 'XGBoost'):\n","    grid = {'n_estimators': [25,50,100,150,200], 'learning_rate': [0.001, 0.005, 0.01],\n","            'max_depth': [2,4,None]}\n","    estimator = xgboost.XGBRegressor(random_state=42)\n","\n","  #Splitting the datasets\n","  if (pred_type == 'single'):\n","    X_train, X_test, y_train, y_test, _ = get_train_test_single(data, name_pred, scaled=scaled)\n","  if ((pred_type == 'topk') or (pred_type == 'topk_avg')):\n","    X_train, X_test, y_train, y_test, _ = get_train_test_extended(data, distances_data, name_pred, pred_type, number_of_cities, scaled=scaled)\n","  if (pred_type == 'all'):\n","    X_train, X_test, y_train, y_test, _ = get_train_test_all(data, scaled=scaled)\n","\n","  #[Train and val using CV] Model predictions on the best parameter combination found by exaustive GridSearch + CV\n","  model_CV, params, runtime, train_mae, train_mase, val_mae, val_mase = best_model_GridSearchCV(\n","      param_grid=grid, X_train=X_train, y_train=y_train, splits=4, base_estimator=estimator)\n","\n","  par_names = [list(params.keys())]\n","  par_values = [list(params.values())]\n","\n","  #[Test set, hold-out]\n","  model_CV.fit(X_train, y_train)\n","  y_pred_train = model_CV.predict(X_train)\n","  y_pred_test = model_CV.predict(X_test)\n","\n","  return y_train, y_test, y_pred_train, y_pred_test"],"metadata":{"id":"wjk8cm4RdS8z","executionInfo":{"status":"ok","timestamp":1711999755426,"user_tz":0,"elapsed":0,"user":{"displayName":"Luiza Lober de Souza Piva","userId":"13136650427747089256"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def get_city_names(df, list_codes):\n","  '''\n","  Quick function to extract names from the unique codes of the disease's datasets.\n","  '''\n","  list_strings = []\n","\n","  for city in list_codes:\n","    filter = df.loc[df['cod_city'] == city]\n","    state = filter['state'].values[0]\n","    name_city = filter['city'].values[0]\n","\n","    list_strings.append(name_city + ', ' + state)\n","\n","  return list_strings"],"metadata":{"id":"LGN7iiXlOK_V","executionInfo":{"status":"ok","timestamp":1711999755426,"user_tz":0,"elapsed":0,"user":{"displayName":"Luiza Lober de Souza Piva","userId":"13136650427747089256"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def generate_figures(disease, model, data, distances_data, number_of_cities,\n","                     list_cities, list_cities_names, save_path):\n","  '''\n","  Generate figures in batches according to the city list provided in list_cities.\n","  '''\n","\n","  counter = 0\n","  for city in list_cities:\n","    #Run forecasts\n","    y_train, y_test, y_pred_train, y_pred_test = display_model(model=model, data=data, distances_data=distances_data,\n","                      data_description='None', pred_type='topk', name_pred=city,\n","                      number_of_cities=number_of_cities, scaled=True)\n","\n","    #Plots and save the resulting figure comparing the dataset and forecasts\n","    plt.plot(list(range(len(y_train), len(y_train) + len(y_test), 1)), y_test.values, label='Actual', color='g')\n","    plt.plot(list(range(len(y_train), len(y_train) + len(y_test), 1)), y_pred_test, label='Prediction', color='orange')\n","    plt.plot(y_train.values, color='g')\n","    plt.plot(y_pred_train, color='orange')\n","    plt.legend(title=list_cities_names[counter], loc='upper left')\n","    plt.axvline(x=len(y_train), color= 'k', ls='--')\n","    plt.savefig(save_path + disease + '_' + model + '_' + str(city) + '.eps', format='eps')\n","    plt.show()\n","\n","    counter = counter +1"],"metadata":{"id":"Bnx7aF7JZNn8","executionInfo":{"status":"ok","timestamp":1711999755443,"user_tz":0,"elapsed":17,"user":{"displayName":"Luiza Lober de Souza Piva","userId":"13136650427747089256"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["# 0) Loading multiple dataframes"],"metadata":{"id":"ScgC7iWg2qg4"}},{"cell_type":"code","source":["#Table with distances between cities.\n","disease = 'dengue'\n","data_path = 'E:/Doutorado/Trabalhos/Projeto-dengue/data/' + disease + '/'\n","\n","df_geo_distances = pd.read_csv(data_path + disease + \"_geo_distances_cities.csv\")\n","\n","#Table with distances between time series calculated with Dynamic Time Warping (DTW), on the data prep notebook\n","df_cases_dtw =  pd.read_csv(data_path + disease +'_distances_cases_dtw.csv')\n","\n","#Another distance table, but using the PIB/GDP values to attribute similarity between the cities\n","df_pib_dtw = pd.read_csv(data_path + disease + '_pib_dtw.csv')\n","\n","#Load the datasets with the first five lags\n","df = pd.read_csv(data_path + disease + '_first_5_lags.csv')\n","df.drop(columns='Unnamed: 0', inplace=True)"],"metadata":{"id":"26WYKRns5OPQ","executionInfo":{"status":"ok","timestamp":1712000455440,"user_tz":0,"elapsed":883,"user":{"displayName":"Luiza Lober de Souza Piva","userId":"13136650427747089256"}}},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":["# 1) Direct predictions on cities"],"metadata":{"id":"zqdlvn8VcBBL"}},{"cell_type":"code","source":["#Parameter to use for the simulation\n","data_to_use = df\n","model_list = ['XGBoost', 'RF']\n","path = 'C:/Users/your_path/'"],"metadata":{"id":"aPr9A-5n_fiX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Run the simulation\n","df_res_cities = pd.DataFrame()\n","\n","state_count = 0\n","num_states = len(data_to_use.state.unique())\n","\n","for state in list(data_to_use.state.unique()):\n","  print('---', state , '---')\n","  print('State', state_count+1, 'of', num_states)\n","  city_count = 0\n","  single_state_data = data_to_use.loc[data_to_use['state'] == state]\n","  num_cities = len(single_state_data['cod_city'].unique())\n","  cities_list = list(single_state_data['cod_city'].unique())    #We'll be using city codes instead of their names\n","\n","  for city in cities_list:\n","    print('---', city, '---')\n","    print('City', city_count+1, 'of', num_cities)\n","    for num_model in range(len(model_list)):\n","      swp = run_model(model=model_list[num_model], data=data_to_use, distances_data=None,\n","                      data_description='Single city', pred_type='single', name_pred=city,\n","                      number_of_cities=None, scaled=True)\n","      #Saves at each step as these calculations can take a while.\n","      df_res_cities = pd.concat([df_res_cities, swp])\n","\n","    city_count += 1\n","  state_count +=1\n","df_res_cities.to_csv(path + disease + '_single_city_seq_lags.csv')"],"metadata":{"id":"jg6G72njpJbR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2) Selecting the top k cities with optimal distance"],"metadata":{"id":"wSAVPtA3QrLU"}},{"cell_type":"markdown","source":["## Top 1"],"metadata":{"id":"bF5MKoe_lK3-"}},{"cell_type":"code","source":["#Parameters to use for all simulation\n","data_to_use = df\n","model_list = ['XGBoost', 'RF']\n","path = 'C:/Users/your_path/'"],"metadata":{"id":"XQkoSIvWrlPy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Geographical distances"],"metadata":{"id":"X0TNvxD2rlP9"}},{"cell_type":"code","source":["data_distances = df_geo_distances         #Table with geographical distances for each city"],"metadata":{"id":"kV3FwG7RrlP9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Run the simulation\n","df_res_cities = pd.DataFrame()\n","\n","state_count = 0\n","num_states = len(data_to_use.state.unique())\n","\n","for state in list(data_to_use.state.unique()):\n","  print('---', state , '---')\n","  print('State', state_count+1, 'of', num_states)\n","  city_count = 0\n","  single_state_data = data_to_use.loc[data_to_use['state'] == state]\n","  num_cities = len(single_state_data['cod_city'].unique())\n","  cities_list = list(single_state_data['cod_city'].unique())    #We'll be using city codes instead of their names\n","\n","  for city in cities_list:\n","    print('---', city, '---')\n","    print('City', city_count+1, 'of', num_cities)\n","    for num_model in range(len(model_list)):\n","      swp = run_model(model=model_list[num_model], data=data_to_use, distances_data=data_distances,\n","                                      data_description='Geo, top 1', pred_type='topk', name_pred=city, number_of_cities=1, scaled=True)\n","      #Saves at each step as these calculations can take a while.\n","      df_res_cities = pd.concat([df_res_cities, swp])\n","\n","    city_count += 1\n","  state_count +=1\n","df_res_cities.to_csv(path + disease + '_top1_Geo_XGBoost.csv')"],"metadata":{"id":"MP6uihc3rlP9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DTW distances on dengue cases"],"metadata":{"id":"4q0uTM4irlP-"}},{"cell_type":"code","source":["data_distances = df_cases_dtw         #Table with geographical distances for each city"],"metadata":{"id":"efwaJ0NlrlP-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Run the simulation\n","df_res_cities = pd.DataFrame()\n","\n","state_count = 0\n","num_states = len(data_to_use.state.unique())\n","\n","for state in list(data_to_use.state.unique()):\n","  print('---', state , '---')\n","  print('State', state_count+1, 'of', num_states)\n","  city_count = 0\n","  single_state_data = data_to_use.loc[data_to_use['state'] == state]\n","  num_cities = len(single_state_data['cod_city'].unique())\n","  cities_list = list(single_state_data['cod_city'].unique())    #We'll be using city codes instead of their names\n","\n","  for city in cities_list:\n","    print('---', city, '---')\n","    print('City', city_count+1, 'of', num_cities)\n","    for num_model in range(len(model_list)):\n","      swp = run_model(model=model_list[num_model], data=data_to_use, distances_data=data_distances,\n","                                      data_description='TS, top 1', pred_type='topk', name_pred=city, number_of_cities=1, scaled=True)\n","      #Saves at each step as these calculations can take a while.\n","      df_res_cities = pd.concat([df_res_cities, swp])\n","\n","    city_count += 1\n","  state_count +=1\n","df_res_cities.to_csv(path + disease + '_top1_TS_XGBoost.csv')"],"metadata":{"id":"9XA4khGpGax6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GDP distances"],"metadata":{"id":"3P6FDXAUrlP_"}},{"cell_type":"code","source":["data_distances = df_pib_dtw         #Table with geographical distances for each city"],"metadata":{"id":"xx7r5c0xrlP_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Run the simulation\n","df_res_cities = pd.DataFrame()\n","\n","state_count = 0\n","num_states = len(data_to_use.state.unique())\n","\n","for state in list(data_to_use.state.unique()):\n","  print('---', state , '---')\n","  print('State', state_count+1, 'of', num_states)\n","  city_count = 0\n","  single_state_data = data_to_use.loc[data_to_use['state'] == state]\n","  num_cities = len(single_state_data['cod_city'].unique())\n","  cities_list = list(single_state_data['cod_city'].unique())    #We'll be using city codes instead of their names\n","\n","  for city in cities_list:\n","    print('---', city, '---')\n","    print('City', city_count+1, 'of', num_cities)\n","    for num_model in range(len(model_list)):\n","      swp = run_model(model=model_list[num_model], data=data_to_use, distances_data=data_distances,\n","                                      data_description='GDP, top 1', pred_type='topk', name_pred=city, number_of_cities=1, scaled=True)\n","      #Saves at each step as these calculations can take a while.\n","      df_res_cities = pd.concat([df_res_cities, swp])\n","\n","    city_count += 1\n","  state_count +=1\n","df_res_cities.to_csv(path + disease + '_top1_GDP_XGBoost.csv')"],"metadata":{"id":"pS-hgelWGaH-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Top 2"],"metadata":{"id":"ZYwjwNuzHFUG"}},{"cell_type":"code","source":["#Parameters to use for all simulation\n","data_to_use = df\n","model_list = ['XGBoost', 'RF']\n","path = 'C:/Users/your_path/'"],"metadata":{"id":"HJLRhMePHFUG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Geographical distances"],"metadata":{"id":"MsomlCi_HFUG"}},{"cell_type":"code","source":["data_distances = df_geo_distances         #Table with geographical distances for each city"],"metadata":{"id":"tXSw7kF7HFUG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Run the simulation\n","df_res_cities = pd.DataFrame()\n","\n","state_count = 0\n","num_states = len(data_to_use.state.unique())\n","\n","for state in list(data_to_use.state.unique()):\n","  print('---', state , '---')\n","  print('State', state_count+1, 'of', num_states)\n","  city_count = 0\n","  single_state_data = data_to_use.loc[data_to_use['state'] == state]\n","  num_cities = len(single_state_data['cod_city'].unique())\n","  cities_list = list(single_state_data['cod_city'].unique())    #We'll be using city codes instead of their names\n","\n","  for city in cities_list:\n","    print('---', city, '---')\n","    print('City', city_count+1, 'of', num_cities)\n","    for num_model in range(len(model_list)):\n","      swp = run_model(model=model_list[num_model], data=data_to_use, distances_data=data_distances,\n","                                      data_description='Geo, top 2', pred_type='topk', name_pred=city, number_of_cities=2, scaled=True)\n","      #Saves at each step as these calculations can take a while.\n","      df_res_cities = pd.concat([df_res_cities, swp])\n","\n","    city_count += 1\n","  state_count +=1\n","df_res_cities.to_csv(path + disease + '_top2_Geo_XGBoost.csv')"],"metadata":{"id":"BeZ2dHTrHFUG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DTW distances on dengue cases"],"metadata":{"id":"Y3S-vYTfHFUG"}},{"cell_type":"code","source":["data_distances = df_cases_dtw         #Table with geographical distances for each city"],"metadata":{"id":"vzQbVFuWHFUG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Run the simulation\n","df_res_cities = pd.DataFrame()\n","\n","state_count = 0\n","num_states = len(data_to_use.state.unique())\n","\n","for state in list(data_to_use.state.unique()):\n","  print('---', state , '---')\n","  print('State', state_count+1, 'of', num_states)\n","  city_count = 0\n","  single_state_data = data_to_use.loc[data_to_use['state'] == state]\n","  num_cities = len(single_state_data['cod_city'].unique())\n","  cities_list = list(single_state_data['cod_city'].unique())    #We'll be using city codes instead of their names\n","\n","  for city in cities_list:\n","    print('---', city, '---')\n","    print('City', city_count+1, 'of', num_cities)\n","    for num_model in range(len(model_list)):\n","      swp = run_model(model=model_list[num_model], data=data_to_use, distances_data=data_distances,\n","                                      data_description='TS, top 2', pred_type='topk', name_pred=city, number_of_cities=2, scaled=True)\n","      #Saves at each step as these calculations can take a while.\n","      df_res_cities = pd.concat([df_res_cities, swp])\n","\n","    city_count += 1\n","  state_count +=1\n","df_res_cities.to_csv(path + disease + '_top2_TS_XGBoost.csv')"],"metadata":{"id":"UOYS5yXZHFUG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GDP distances"],"metadata":{"id":"1XC_yTw7HFUG"}},{"cell_type":"code","source":["data_distances = df_pib_dtw         #Table with geographical distances for each city"],"metadata":{"id":"5xUhDBhhHFUG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Run the simulation\n","df_res_cities = pd.DataFrame()\n","\n","state_count = 0\n","num_states = len(data_to_use.state.unique())\n","\n","for state in list(data_to_use.state.unique()):\n","  print('---', state , '---')\n","  print('State', state_count+1, 'of', num_states)\n","  city_count = 0\n","  single_state_data = data_to_use.loc[data_to_use['state'] == state]\n","  num_cities = len(single_state_data['cod_city'].unique())\n","  cities_list = list(single_state_data['cod_city'].unique())    #We'll be using city codes instead of their names\n","\n","  for city in cities_list:\n","    print('---', city, '---')\n","    print('City', city_count+1, 'of', num_cities)\n","    for num_model in range(len(model_list)):\n","      swp = run_model(model=model_list[num_model], data=data_to_use, distances_data=data_distances,\n","                                      data_description='GDP, top 2', pred_type='topk', name_pred=city, number_of_cities=2, scaled=True)\n","      #Saves at each step as these calculations can take a while.\n","      df_res_cities = pd.concat([df_res_cities, swp])\n","\n","    city_count += 1\n","  state_count +=1\n","df_res_cities.to_csv(path + disease + '_top2_GDP_XGBoost.csv')"],"metadata":{"id":"hrQFlJP6HFUG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Top 3"],"metadata":{"id":"c_2Vj4ruHGHI"}},{"cell_type":"code","source":["#Parameters to use for all simulation\n","data_to_use = df\n","model_list = ['XGBoost', 'RF']\n","path = 'C:/Users/your_path/'"],"metadata":{"id":"mAYkpZqdHGHZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Geographical distances"],"metadata":{"id":"vMK6UT71HGHZ"}},{"cell_type":"code","source":["data_distances = df_geo_distances         #Table with geographical distances for each city"],"metadata":{"id":"uUq0fg8jHGHZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Run the simulation\n","df_res_cities = pd.DataFrame()\n","\n","state_count = 0\n","num_states = len(data_to_use.state.unique())\n","\n","for state in list(data_to_use.state.unique()):\n","  print('---', state , '---')\n","  print('State', state_count+1, 'of', num_states)\n","  city_count = 0\n","  single_state_data = data_to_use.loc[data_to_use['state'] == state]\n","  num_cities = len(single_state_data['cod_city'].unique())\n","  cities_list = list(single_state_data['cod_city'].unique())    #We'll be using city codes instead of their names\n","\n","  for city in cities_list:\n","    print('---', city, '---')\n","    print('City', city_count+1, 'of', num_cities)\n","    for num_model in range(len(model_list)):\n","      swp = run_model(model=model_list[num_model], data=data_to_use, distances_data=data_distances,\n","                                      data_description='Geo, top 3', pred_type='topk', name_pred=city, number_of_cities=3, scaled=True)\n","      #Saves at each step as these calculations can take a while.\n","      df_res_cities = pd.concat([df_res_cities, swp])\n","\n","    city_count += 1\n","  state_count +=1\n","df_res_cities.to_csv(path + disease + '_top3_Geo_XGBoost.csv')"],"metadata":{"id":"xOP_l62sHGHZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DTW distances on dengue cases"],"metadata":{"id":"nnFy2yLEHGHZ"}},{"cell_type":"code","source":["data_distances = df_cases_dtw         #Table with geographical distances for each city"],"metadata":{"id":"cNf2TKDPHGHZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Run the simulation\n","df_res_cities = pd.DataFrame()\n","\n","state_count = 0\n","num_states = len(data_to_use.state.unique())\n","\n","for state in list(data_to_use.state.unique()):\n","  print('---', state , '---')\n","  print('State', state_count+1, 'of', num_states)\n","  city_count = 0\n","  single_state_data = data_to_use.loc[data_to_use['state'] == state]\n","  num_cities = len(single_state_data['cod_city'].unique())\n","  cities_list = list(single_state_data['cod_city'].unique())    #We'll be using city codes instead of their names\n","\n","  for city in cities_list:\n","    print('---', city, '---')\n","    print('City', city_count+1, 'of', num_cities)\n","    for num_model in range(len(model_list)):\n","      swp = run_model(model=model_list[num_model], data=data_to_use, distances_data=data_distances,\n","                                      data_description='TS, top 3', pred_type='topk', name_pred=city, number_of_cities=3, scaled=True)\n","      #Saves at each step as these calculations can take a while.\n","      df_res_cities = pd.concat([df_res_cities, swp])\n","\n","    city_count += 1\n","  state_count +=1\n","df_res_cities.to_csv(path + disease + '_top3_TS_XGBoost.csv')"],"metadata":{"id":"URlfMYeFHGHZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GDP distances"],"metadata":{"id":"rvhht-XUHGHZ"}},{"cell_type":"code","source":["data_distances = df_pib_dtw         #Table with geographical distances for each city"],"metadata":{"id":"w9IT_XwzHGHZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Run the simulation\n","df_res_cities = pd.DataFrame()\n","\n","state_count = 0+6\n","num_states = len(data_to_use.state.unique())\n","\n","for state in list(data_to_use.state.unique()):\n","  print('---', state , '---')\n","  print('State', state_count+1, 'of', num_states)\n","  city_count = 0\n","  single_state_data = data_to_use.loc[data_to_use['state'] == state]\n","  num_cities = len(single_state_data['cod_city'].unique())\n","  cities_list = list(single_state_data['cod_city'].unique())    #We'll be using city codes instead of their names\n","\n","  for city in cities_list:\n","    print('---', city, '---')\n","    print('City', city_count+1, 'of', num_cities)\n","    for num_model in range(len(model_list)):\n","      swp = run_model(model=model_list[num_model], data=data_to_use, distances_data=data_distances,\n","                                      data_description='GDP, top 3', pred_type='topk', name_pred=city, number_of_cities=3, scaled=True)\n","      #Saves at each step as these calculations can take a while.\n","      df_res_cities = pd.concat([df_res_cities, swp])\n","\n","    city_count += 1\n","  state_count +=1\n","df_res_cities.to_csv(path + disease + '_top3_GDP_XGBoost.csv')"],"metadata":{"id":"3gvt07rDHGHZ"},"execution_count":null,"outputs":[]}]}